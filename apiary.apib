FORMAT: 1A

# Data Wrangling: Pandas vs. Pyspark DataFrame
    +   This basic introduction is to compare common data wrangling methods in pyspark and pandas data frame with a concrete example. 
        Here, I used US counties' COVID-19 dataset to show the data wrangling differences between these two types of data frames. 
        The dataset can be downloaded from Kaggle Dataset (https://www.kaggle.com/fireballbyedimyrnmom/us-counties-covid-19-dataset). 
        This should allow you to get started with data manipulation and analysis under both pandas and spark. 
        Specific objectives are to show you how to:
        Load data from local files

# Download sample data (covid-19-data/us-counties.csv) from internet
%sh wget https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv -O /databricks/driver/us-counties.csv

import pandas as pd 
#import findspark
#findspark.init('/opt/spark')

import pyspark
import random
#from pyspark import SparkContext, SparkConf, SQLContext
#from pyspark.sql import SparkSession

from pyspark.sql.functions import udf 
from pyspark.sql.types import StructField,IntegerType, StructType,StringType, FloatType
from pyspark.sql.functions import desc 
from pyspark.sql.functions import asc 
import pyspark.sql.functions as F 
import numpy as np
from pyspark.sql import Window
from pyspark.sql.functions import isnan, when, count, col
from pyspark.sql.types import DateType
from pyspark.sql.functions import *

# Load data from local files
# Pandas 
df = pd.read_csv(local_file)

# Pyspark 
df_s = spark.read.csv(dbfs_file,header = True)

# Display the schema of the DataFrame
## pandas 
print(df.dtypes)

## pyspark 
print(df_s.printSchema())
df

# Change the Data Types when Importing the Data
# When we check the data types above, we found that the cases and deaths need to be converted to numerical values instead of string format. Here is the way how we can change the data types when importing the data in pandas and spark.

# pandas : aim to reduce memory size 

types = {'date': 'object', 'county': 'object', 
         'state': 'object', 'fips': 'float',
         'cases': pd.Int64Dtype(), 'deaths': pd.Int64Dtype()}

df = pd.read_csv(local_file, usecols=types.keys(), dtype=types)
#df = pd.read_csv(dbfs_file, usecols=types.keys(), dtype=types)

# pyspark 
newDF=[StructField('date',StringType(),True),
       StructField('county',StringType(),True),
       StructField('state',StringType(),True),
       StructField('fips',FloatType(),True),
       StructField('cases',IntegerType(),True),
       StructField('deaths',IntegerType(),True)
       ]
finalStruct=StructType(fields=newDF)
df_s = spark.read.csv(dbfs_file,header = True,schema=finalStruct)
df_s.printSchema()

print(df.info(memory_usage='deep'))

print(df_s.rdd.getNumPartitions())

# %md ###[Data Files, File Split, Partition](https://medium.com/swlh/building-part ...

# Show the head of the DataFrame

# pandas 
df.head()

# pyspark 
df_s.show(5)

df_s.take(5)
 
# In pyspark, take() and show() are different. show() prints results, take() returns a list of rows (in PySpark) and can be used to create a new dataframe. They are both actions.

display(df_s)

# Select Columns from the DataFrame

## Pandas 
df[['state','cases']].head()

## pyspark 
df_s.select('state','cases').show(5)

# Show the Statistics of the DataFrame

# pandas 
df.describe() # show the stats for all numerical columns 
df['cases'].describe() # show the stats for a specific column

# pyspark 
df_s.describe().show() #pyspark can show stats for all columns but it may contain missing values 
df_s.describe('cases').show() # show the stats for a specific column

# Drop Duplicates
# pandas 
df.state.drop_duplicates().head()

# pyspark 
df_s.select('state').dropDuplicates().show(5)